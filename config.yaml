# 0: background, 1: ich, 2: ivh, 3: edh,sdh, 4: sah
general:
  img_shape: [512, 512]
  class_list: [1, 2]
  cuda_device: '1'  # gpu select : 0 or 1
  label_names: ['0:BG', '1:ICH', '2:IVH', '3:EXTRA', '4:SAH', '5:ISDH']
  threshold: 0.5

model:
  TransUnet:
    model_name: TransUnet
    vit_name: R50-ViT-B_16
    vit_patches_size: 16
    n_skip: 3
    pretrained_weights: TransUNet/vit_models/imagenet21k+imagenet2012_R50+ViT-B_16.npz
  UTNetV2:
    model_name: UTNetV2
    pretrained_weights: 

dataset:
  trainset_path: /home/yhkim/workspace/project-CH-Labeling/labeld_dataset/labeled_dset4_220511/trainset
  validset_path: /home/yhkim/workspace/project-CH-Labeling/labeld_dataset/labeled_dset4_220511/validset
  pixel_limit: 80
  bg_train_ratio: 0.5  # 학습시 non_CH 이미지 포함비율(에포크마다 비율만큼 랜덤으로 샘플링)
  oversampling_values: {2: 1}  # 오버샘플링시 사용

trainer:
  log_path: test1/test_2  # 로그 저장 경로
  base_lr: 5.0e-3
  target_iteration: 20000
  min_chkpoint_iteration: 1000  # 해당 이터레이션 이후 모델 체크포인트 저장
  per_log_iter: 150  # 해당 iteration마다 로그저장
  train_batch_size: 6
  valid_batch_size: 12
  train_sample_idx: 610  # prediction 샘플 번호
  valid_sample_idx: 72  

  optimizer:
    SGD:
      momentum: 0.9
      weight_decay: 0.0001
    AdamW:
      weight_decay: 0.01

  scheduler:
    CosineAnnealingWarmRestarts:
      T_0: 150
      T_mult: 1
      eta_min: 0
      last_epoch: -1
    ExpTargetIterScheduler:  # original TransUnet Scheduler, utils.py에 구현
      gamma: 0.9
      min_lr: 1.0e-7
      target_iteration: 20000  # lr이 0되는 지점
    StepLR:
      step_size: 500
      gamma: 0.9
      last_epoch: -1

  loss_value:  # 현재 0.5*ce_loss + 0.5*dice_loss 사용중(TransUnet default values)
    ce_loss: 0.5
    dice_loss: 0.5